> [原文地址](https://blog.csdn.net/qq_36426650/article/details/112222115)

# 【预训练语言模型】Attention Is All You Need（Transformer）
