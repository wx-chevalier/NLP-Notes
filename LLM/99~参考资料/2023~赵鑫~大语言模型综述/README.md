# 大语言模型综述

# 摘要

自从 20 世纪 50 年代图灵测试被提出以来，人类一直在探索如何用机器掌握语言智能。语言本质上是一种由语法规则支配的复杂的人类表达系统。开发有能力理解和掌握一门语言的人工智能（AI）算法是一个重大挑战。作为一种主要的语言理解和生成方法，语言建模在过去的二十年中得到了广泛的研究，并从统计语言模型逐步发展为神经语言模型。近年来，通过在大规模语料库上对 Transformer 模型进行预训练，人们提出了预训练语言模型（PLM），其在解决各种自然语言处理（NLP）任务方面表现出强大的能力。由于研究人员发现扩展模型规模可以提高模型能力，因此他们通过将参数增加到更大的尺寸来进一步研究该效应。有趣的是，当参数规模超过一定水平时，这些规模更大的语言模型的性能不仅得到了显著提升，而且还表现出一些小规模语言模型（例如 BERT）所不具备的特殊能力（例如上下文学习）。为了区分不同参数规模下的语言模型，研究界创造了术语——大语言模型（LLM）代指大型的 PLM（如包含数百亿或数千亿个参数）。近年来，学术界和工业界极大地推进了针对 LLM 的研究，其中一个显著的进展是推出了 ChatGPT（一种基于 LLM 开发的强大 AI 聊天机器人），它引起了社会的广泛关注。LLM 的技术发展对整个 AI 界产生了重要影响，这将彻底改变我们开发和使用 AI 算法的方式。考虑到这一快速的技术进步，在本篇综述中，我们通过介绍 LLM 的背景、主要发现和主流技术来回顾近年来的进展。我们特别关注 LLM 的四个主要方面，即预训练、适配微调、使用和能力评估。此外，我们还总结了开发 LLM 的可用资源，并讨论了 LLM 现有的问题和未来的发展方向。本文提供了关于 LLM 的最新文献综述，期望能为研究人员和工程师提供帮助
